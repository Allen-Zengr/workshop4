== Application Deployment Configurations and DevOps Approaches

=== Introduction

*DevOps is a much overloaded term that is used to describe a variety of concepts in the creation of modern applications. In a simple definition DevOps is concerned with the interface between development practices used for the creative elements of software engineering and the procedural rigour of operationally running applications in production. Clearly these concepts have different objectives so it is important for teams (whether tasked with development or operations) to have a good understanding of the concepts, objectives and concerns of their counterparts on the other side of the fence.*

*In terms of this workshop it is important to highlight the  capabilities of containerised platforms with respect to the roll out of new versions of applications and how they get introduced to production and to end users. Terms such as blue/green deployments, black and white deployments, A/B deployments and canary deployments are used in various text books on 'DevOps' principles and some of these areas will be used in this chapter of the workshop.*

*The activities in this workshop will introduce a new version of a simple application to use in two different manners.*

==== Workshop Content

*The application to be used in this workshop is a simple NodeJS REST interface. Version 1 exists on the master branch of the GIT Repository and verison 2 exists on the experimental branch.*

*To begin the creation of the application use the following commands in an command line window. :*

==== Creation of version 1

oc new-project master-slaveX 

*(Where X is your user number)*

oc new-app --name slave-app-v1-0  https://github.com/marrober/slave-node-app.git

oc expose service slave-app-v1-0 --name="slave-app-route"

.What got created ?
****

It is important to understand the artefacts and content that are created by the above commands within OpenShift:

* A new project is created called master-slaveX.
* A deployment configuration is created called slave-app-v1-0.
* A replication controller is created associated with the application and the built objects. This defines the scaling of the application in terms of the number of pods and the deployment strategy (rolling or recreate). 
* A build configuration is created to compile the application source code which is extracted from the associated GIT repository. 
* The build configuration will create a buid instance which includes logs of the build process and a reference to the built image.
* The newly created image is deployed as a runnning pod (or pods depending on the number of pods defined in the deployment configuration).
* A service is created as part of the application creation process. The service endpoints will point to the pod(s) created by the build process. In the case of a rebuild and redeploy operation the service endpoints are updated to point to the new pods in accordance with the deployment strategy of the deployment configuration.
* The service is then exposed external to the cluster by the creation of a route. The route has a fully qualified domain name for access from machines outside the cluster. 

The diagram below shows the above and how the relate together.

image::deployment-strategies-1.png[Deployment artefacts]
****

*To identify the URL of the route execute the command shown below:*

[source,shell]
----
oc get route slave-app-route -o jsonpath='{"http://"}{.spec.host}{"/ip\n"}'
----

*This will display a formatted URL with the 'http://' part at the beginning which will be similar to :*

http://slave-app-route-master-slave.apps.cluster-xxxx-yyyy.xxxx-yyyy.example.opentlc.com

*To test the application use the command line window to issue a curl command to:


curl -k <url from the above command>


*The response should be as shown below (example ip address) :*

"10.131.0.114 v1.0"


==== Creation of version 2

*The development team now wants to introduce an experimental version 2 of the application and introduce it to the users in a number of different ways. The first action is to create the new build process for the experimental version using the command below.*

oc new-app --name slave-app-v2-0 https://github.com/marrober/slave-node-app.git#experimental

*Note the use of the #experimental branch identifier in the end of the GIT repository. This syntax cannot be used through the web interface to openshift so any requirements for non-standard GIT connectivity must be done through the command line interface. It is also possible to configure the source-2-image capability to reference a specific sub directory of the repository using the --context option.*

.What got added to the project ?
****

New content was added to the project as a result of the above command specific to the experimental (v2) version of the application.

* A deployment configuration
* A replication controller 
* A build configuration
* A running container in a pod
* A service

The diagram below shows the above and how the relate together.

image::deployment-strategies-2.png[Deployment artefacts]
****

*At this point the version 2 application is operational but not accessible externally to the cluster.*

=== Blue / Green Deployment

*The benefit of creating the new version of the application alongside the old is that it is quick and easy to migrate users to a new version of the application. It also allows teams to validate that the pods are running correctly.*

*Switching the route from v1 to v2 involves patching the route to change configuration. Before executing this operation open a new command window and execute the command below to send requests to the route every second. The responses received should all include the ip address of the pod and the version (v1) of the application.*

*Get the URI of the route using the command :*

[source,shell]
----
oc get route slave-app-route -o jsonpath='{"http://"}{.spec.host}{"/ip\n"}'
----

*Copy the result of the above command and past it into the shell script below:*


[source,shell]
----
for i in {1..1000}; do curl -k <URI> ; echo ""; sleep 1; done
----

*A series of reports of ip address and version 1 of the application will then start to scroll up the screen. Leave this running.*

*Switch back to the original command window and execute the command below to patch the route to version 2 of the application.*

[source,shell]
----
oc patch route/slave-app-route -p '{"spec":{"to":{"name":"slave-app-v2-0"}}}'
----

*Switch back to the command window with the shell script running and you should see the responses have a new ip address and now report v2 of the application. This has completed a migration from the old version of the application to the new.*

*The details of the route patched by the above command are displayed by the command:*

oc get route/slave-app-route -o yaml

*The output of the above command is shown below, and the nested information from spec -> to -> name is easy to see.*

[source,shell]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    openshift.io/host.generated: "true"
  creationTimestamp: 2019-12-04T17:16:37Z
  labels:
    app: slave-app-v1-0
  name: slave-app-route
  namespace: master-slave
  resourceVersion: "884652"
  selfLink: /apis/route.openshift.io/v1/namespaces/master-slave/routes/slave-app-route
  uid: d4910fef-16b9-11ea-a6c5-0a580a800048
spec:
  host: slave-app-route-master-slave.apps.cluster-telf-c8e6.telf-c8e6.example.opentlc.com
  port:
    targetPort: 8080-tcp
  subdomain: ""
  to:
    kind: Service
    name: slave-app-v2-0
    weight: 100
  wildcardPolicy: None
status:
  ingress:
  - conditions:
    - lastTransitionTime: 2019-12-04T17:16:38Z
      status: "True"
      type: Admitted
    host: slave-app-route-master-slave.apps.cluster-telf-c8e6.telf-c8e6.example.opentlc.com
    routerCanonicalHostname: apps.cluster-telf-c8e6.telf-c8e6.example.opentlc.com
    routerName: default
    wildcardPolicy: None
----

*Before moving to the A/B deployment strategy switch back to version v1 with the command:*
[source,shell]
----
oc patch route/slave-app-route -p '{"spec":{"to":{"name":"slave-app-v1-0"}}}'
----

*Confirm this has worked in the command window executing the shell script.*

=== A/B Deployment

*The benefit of an A/B deployment strategy is that it is possible to gradually migrate workload to the new version. This example presents a simple process of gradually migrating a higher and higher percentage of traffic to the new version, however more advanced options are available for migrating traffic based on headers or source ip address to name just two. Red Hat OpenShift Service Mesh is another topic that is worth investigation if advanced traffic routing operations are required.*

*Gradually migrating traffic fromv1 to v2 involves patching the route to change configuration as shown below.*

image::deployment-strategies-3.png[Traffic routing]

*To migrate 10% of traffic to version 2 execute the following command:*.

oc set route-backends slave-app-route slave-app-v1-0=90 slave-app-v2-0=10

*Switch back to the command window running the shell script and after a short wait you will see the occasional report from version 2.*

*To balance the workload between the two versions execute the following command:*

oc set route-backends slave-app-route slave-app-v1-0=50 slave-app-v2-0=50

*Switch back to the command window running the shell script and after a short wait you will see a more even distribution of calls between versions 1 and 2.*

*The details of the route patched by the above command are displayed by the command:*

oc get route/slave-app-route -o yaml

*A section of the output of the above command is included below, showing the split of traffic between versions 1 and 2.*

[source,shell]
----
spec:
  alternateBackends:
  - kind: Service
    name: slave-app-v2-0
    weight: 50
  host: slave-app-route-master-slave.apps.cluster-telf-c8e6.telf-c8e6.example.opentlc.com
  port:
    targetPort: 8080-tcp
  subdomain: ""
  to:
    kind: Service
    name: slave-app-v1-0
    weight: 50
----

*When satisfied that version 2 is working as required the following command will switch all traffic to that version and will remove the references to version 1 from the route.*

oc set route-backends slave-app-route slave-app-v1-0=0 slave-app-v2-0=100

==== Cleaning up

From the OpenShift browser window click on 'Advanced' and then 'Projects' on the left hand side menu.

In the triple dot menu next to your own project (master-slaveX) select ‘Delete Project’
Type ‘master-slaveX’ (where X is your user number) such that the Delete button turns red and is active.

Press Delete to remove the project.
